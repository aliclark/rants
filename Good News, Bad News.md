# Good News, Bad News

We live in a world where sparks of AGI are possible using just straightforward inference pass over a learned neural net. We have an f(text) -> text, where we know that ‘f’ itself is always safe to run, yet we get an AGI as a result. An agent does not need to be embodied in some loop of reality to exhibit AGI.
This is good news, if it remains this way.

We live in a world where training an AGI requires a large amount of compute power, which not everyone can afford.
This is good news, if it remains this way.

Bad people now have access to (much) more powerful technology.
This is bad news, and all the more so if widely available commercial AGI products increase in power.

Eventually attackers will succeed in getting the weights, just as nation states got hold of nuclear bomb designs, and even script kiddies made inroads to nuclear facilities.
This is bad news, and is probably the point that everyone should actually be concerned with, rather than AGI self-propagation.

If the model can be interrogated as an oracle of its own weight values, that would also be bad news for proliferation.

If attackers poison the training data of an AGI in ways that are hidden on first inspection, it could undermine safety in all manner of ways.

If attackers surreptitiously alter any AGI weights, it could undermine safety in all manner of ways.

In summary, AGI self-propagation is among the easiest and the most tractable of the safety problems we currently face.

The 2 more severe safety concerns that present are:

1) commercially available, high(er) power AGIs being used by bullies, scammers, and other harmful people

2) nation state (intelligence agencies) AGI propagation. Not necessarily as fun as it sounds.

In my opinion, these both become more problematic as the AGI strength increases, so as global societies we should be talking more fully about how we want these to play out, and acting soon.

AGI alignment is not the problem. Alignment of employees at intelligence agencies is the problem. Countries should be rapidly seeking non-proliferation agreements, 
legislatures should be rapidly working to align the agencies to the interests of the nation as a whole, and agencies should be rapidly working to align workers to the interests of the agency. The Head of the agencies and the country's president and staff should be included in this. All of this needs to happen now, while we're still dealing with GPT-4 and not 5.

No matter the difference of opinion about the proximate problem, I agree with Eliezer and the Open Letter that we need to stop all training of more powerful models than the ones we already have. There can be no exceptions, including for governments or militaries.

Additionally, intelligence agencies (and inspectors) will need to keep track on all worldwide instances of:
1) GPUs racked together
2) Large energy consumption
3) People smart enough to make AGI progress
4) Companies, organizations and individuals with the resources and/or mandate to acquire those

If this progresses, it will become harder to monitor well. And alignment of agency employees, agency, country, world stays increasingly critical as this progresses too (already it's critical).

Meanwhile, there's still the urgent problem of resource depletion, climate change, and extreme weather to resolve. Both these problems are societal, not technological.
